{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InT7_Hnn-_4-"
   },
   "source": [
    "# Using YOLOv3 and OpenCV 4 to detect objects\n",
    "\n",
    "I wanted to try to use OpenCV and YoloV3 in order to detect basics objects, this is what this notebook will be about.\n",
    "\n",
    "*Thank you Joseph Redmon and Ali Farhadi for their incredible work on YOLO object detector* : **YOLOv3: An Incremental Improvement**, *Redmon, Joseph and Farhadi, Ali* ; arXiv, 2018.\n",
    "\n",
    "This paper was my main inspiration for this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VgFkipuK-_5E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version :  4.5.1\n"
     ]
    }
   ],
   "source": [
    "# import useful libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "print('OpenCV version : ', cv2. __version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J011UaOx-_5H"
   },
   "source": [
    "### Load YoloV3 models with pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b-_j5ie8-_5H"
   },
   "outputs": [],
   "source": [
    "# function to load our classes names\n",
    "def read_classes(file):\n",
    "    \"\"\" Read the classes files and extract the classes' names in it\"\"\" \n",
    "    classNames = []    \n",
    "    with open(file, 'rt') as f:\n",
    "        classNames = f.read().rstrip('\\n').split('\\n')\n",
    "   \n",
    "    return classNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lNsDmSE-_5I",
    "outputId": "9915621d-8c15-417f-ea2e-93befabc392c"
   },
   "outputs": [],
   "source": [
    "# test our function read_classes\n",
    "img_file = './data/coco.names'\n",
    "classNames = read_classes(img_file)\n",
    "# classNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17jo_5tzLNiZ"
   },
   "source": [
    "### YoloV3 weights and cfg files\n",
    "\n",
    "In this part we'll upload YoloV3 Weights and cfg files in order to input them into cv2.dnn.readNetFromDarknet() and build our forward propagation with OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "sYmLADJ2LNSA",
    "outputId": "aba64119-7ff5-4336-a35f-acc1016a9800"
   },
   "outputs": [],
   "source": [
    "# load the model config and weights\n",
    "modelConfig_path = './cfg/yolov3.cfg'\n",
    "modelWeights_path = './models/yolov3-320.weights'\n",
    "\n",
    "# read the model cfg and weights with the cv2 DNN module\n",
    "neural_net = cv2.dnn.readNetFromDarknet(modelConfig_path, modelWeights_path)\n",
    "# set the preferable Backend to GPU\n",
    "neural_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "neural_net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "# confidence and non-max suppression threshold for this YoloV3 version\n",
    "confidenceThreshold = 0.5\n",
    "nmsThreshold = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCoGUKRILNN1"
   },
   "source": [
    "## Function creation for object detection\n",
    "\n",
    "In this part, we'll define the object detection function which allows us to get **the output of the three YOLO layers (82,94,106)** and select **the best bounding box** to put on the detected object, the layers' outputs are in the format :\n",
    "- The confidence probability (if there is an object or not in the frame)\n",
    "- (x,y) coordinates of the center of the object\n",
    "- (height, width) of the frame\n",
    "- All the scores of the detected object (if there is an object detected, which one is it?)\n",
    "\n",
    "We know that the first 5 elements of the results ouputs are not the \"scores\", **i.e.** : *the 80 classes probabilities of what kind of object is detected*. That's why we'll start to store our outputs layers' values in different lists, the first one will stock all the **scores** from the fifth element to the last element (80th), then we'll collect the first 5 elements. \n",
    "\n",
    "For the 5th to 80th elements corresponding to *the objects scores*, we'll take the maximum value which will represent the **highest probability** about what's the object's class.\n",
    "\n",
    "#### Object detection function\n",
    "\n",
    "Based on what we've detailed above, we'll write an object detection function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(outputs, input_frame):\n",
    "    \"\"\" This function will allow us to draw bounding boxes on detected objects in frames (images) \"\"\"\n",
    "    # first we'll collect the height, width and channel of the input frame (3 if the image is RGB, 1 if it's grayscale)\n",
    "    height, width, channel = input_frame.shape\n",
    "    \n",
    "    # we'll create category lists to store the layers' output values \n",
    "    bounding_boxes = []\n",
    "    class_objects = []\n",
    "    confidence_probs = []\n",
    "    \n",
    "    # Knowing that there are 3 YOLO layers, we'll browsing them and their outputs using this :\n",
    "    for result in outputs:        \n",
    "        for values in result:\n",
    "            \n",
    "            scores = values[5:] # we know that the class probabilities are from the 5th values\n",
    "            indices_object = np.argmax(scores) # get the indice of the max score\n",
    "            confidence_probability = scores[indices_object] # store the maximum value of the indice found\n",
    "            \n",
    "            # in order to have a proper detection, we'll eliminate the weakest probability detection by imposing a threshold\n",
    "            if confidence_probability > confidenceThreshold:   \n",
    "                \n",
    "                # get the pixel values corresponding to the scaling of the bounding box coordinates to the initial frame\n",
    "                box_detected = values[0:4] * np.array([width, height, width, height])                                 \n",
    "                # get the top left corner coordinates by extracting values from box_detected and perform calculations\n",
    "                x, y, w, h = box_detected\n",
    "                # we're converting the coordinates to int because OpenCV doesn't allow floats for bounding boxes\n",
    "                x = int(x - (w/2))\n",
    "                y = int(y - (h/2))\n",
    "                \n",
    "                # adding the good detected boxe in the bounding boxes list created\n",
    "                bounding_boxes.append([x,y,w,h])                \n",
    "                # adding the detected objects indices in the class objects list \n",
    "                class_objects.append(indices_object)                \n",
    "                # adding the max value of the object score (confidence) in the confidences_probs list\n",
    "                confidence_probs.append(float(confidence_probability))    \n",
    "                    \n",
    "    return bounding_boxes, class_objects, confidence_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-max suppresion function\n",
    "\n",
    "When we're doing object detection with YoloV3, the algorithm outputs several bounding boxes. In order to keep the best bounding box we'll need to perform a maximizimation technique called **non-max suppression**, this method will allow us to keep the bounding box with the highest probability and eliminate the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_bbox(bounding_boxes, confidence_probs, confidenceThreshold, nmsThreshold):\n",
    "    \"\"\"This function performs non-max suppression on all the bounding boxes detected and keeps the best one\"\"\"    \n",
    "    #Using OpenCV DNN non-max supression to get the best bounding box of the detected object (retrieve the indices)\n",
    "    indices_bbox = cv2.dnn.NMSBoxes(bounding_boxes, confidence_probs, confidenceThreshold, nmsThreshold)        \n",
    "    print('Number of objects detected : ', len(indices_bbox), '\\n')\n",
    "    \n",
    "    return indices_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawing box function\n",
    "\n",
    "Now, we have only one bounding box left (the best one) and we'll just need to draw the corresponding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_drawing(input_frame, indices, bounding_boxes, class_objects, confidence_probs, classNames, color=(0,255,255), thickness=2):\n",
    "    \"\"\" Drawing the detected objects boxes \"\"\"\n",
    "    # once we have the indices, we'll extract the values of x,y,w,h of the best bounding boxes and stores it.\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        final_box = bounding_boxes[i]\n",
    "    # we'll retrieve the bounding boxes values (coordinates) now and use them to draw our boxes.\n",
    "        x, y, w, h = final_box[0], final_box[1], final_box[2], final_box[3]\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)        \n",
    "        print('Bounding box coordinates in the frame : ', 'x : ', x,'|| y : ',y,'|| w : ',w,'|| h :',h , '\\n')\n",
    "    \n",
    "        cv2.rectangle(input_frame, (x,y), (x+w,y+h),  (0, 255, 255), 2)\n",
    "        cv2.putText(input_frame, f'{classNames[class_objects[i]].upper()} {int(confidence_probs[i]*100)}%',\n",
    "                        (x, y-10), cv2.FONT_HERSHEY_DUPLEX, 0.6,  (0, 255, 255), 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Blob function \n",
    "\n",
    "OpenCV uses blob techniques to extract features from an image and allows detection. In order to be able to do forward propagation on our images or videos, we'll need to convert our images to **OpenCV blob's format**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_blob(input_frame, network, height=320, width=320):\n",
    "    \"\"\" This function allow us to convert a frame/image into blob format for OpenCV DNN\"\"\"    \n",
    "    blob = cv2.dnn.blobFromImage(input_frame, 1/255, (height,width), [0,0,0], 1, crop=False)\n",
    "    network.setInput(blob)\n",
    "    # get the YOLO output layers numbers (names), these layers will be useful for the detection part\n",
    "    # the layer's name : yolo_82, yolo_94, yolo_106\n",
    "    yoloLayers = network.getLayerNames()\n",
    "    outputLayers = [(yoloLayers[i[0]-1]) for i in network.getUnconnectedOutLayers()]\n",
    "    # Doing forward propagation with OpenCV\n",
    "    outputs = network.forward(outputLayers)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to load our image from directory\n",
    "import os \n",
    "\n",
    "def load_image(image):    \n",
    "    \"\"\" Return the video by inputing the title of the video \"\"\" \n",
    "\n",
    "    # get the video path and load the video\n",
    "    image_path = os.path.join('images', image)\n",
    "    \n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects detected :  3 \n",
      "\n",
      "Bounding box coordinates in the frame :  x :  187 || y :  91 || w :  89 || h : 289 \n",
      "\n",
      "Bounding box coordinates in the frame :  x :  67 || y :  266 || w :  137 || h : 81 \n",
      "\n",
      "Bounding box coordinates in the frame :  x :  392 || y :  139 || w :  208 || h : 206 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = neural_net\n",
    "height, width = 320,320\n",
    "\n",
    "img = cv2.imread(load_image('person1.jpg'))\n",
    "\n",
    "# using convert_to_blob function : \n",
    "outputs = convert_to_blob(img, network)    \n",
    "# apply object detection on the video file\n",
    "bounding_boxes, class_objects, confidence_probs = object_detection(outputs, img)   \n",
    "# perform non-max suppression\n",
    "indices = nms_bbox(bounding_boxes, confidence_probs, confidenceThreshold, nmsThreshold)\n",
    "# draw the boxes\n",
    "box_drawing(img, indices, bounding_boxes, class_objects, confidence_probs, classNames, color=(0,255,255), thickness=2)\n",
    "\n",
    "cv2.imshow('Object detection in images', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "yolov3_ftn4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
